# ğŸ”§ SoluÃ§Ã£o para Erro de Limite de Tokens

## âŒ Problema Original

Ao tentar processar o documento `Passo a passo - MÃ³dulo de Compras_documentacao_gerada.md`, vocÃª recebeu o seguinte erro:

```
Error code: 400 - {'error': {'message': 'Requested 338717 tokens, max 300000 tokens per request', 'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}
```

**Causa:** O documento gerado Ã© muito grande (338,717 tokens) e excede o limite mÃ¡ximo da API OpenAI (300,000 tokens por requisiÃ§Ã£o).

---

## âœ… SoluÃ§Ã£o Implementada

### 1. **Contagem Precisa de Tokens**

Adicionei a biblioteca `tiktoken` (jÃ¡ instalada) para contar tokens com precisÃ£o:

```python
def count_tokens(text: str, model: str = "gpt-4o-mini") -> int:
    """
    Conta o nÃºmero de tokens em um texto usando tiktoken.
    Se tiktoken nÃ£o estiver disponÃ­vel, usa estimativa (1 token â‰ˆ 4 caracteres).
    """
    if TIKTOKEN_AVAILABLE:
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except Exception:
            pass
    
    # Estimativa: ~4 caracteres por token (regra geral)
    return len(text) // 4
```

### 2. **DivisÃ£o Inteligente de Documentos Grandes**

Implementei uma funÃ§Ã£o que divide documentos grandes em partes menores, respeitando a estrutura markdown:

```python
def split_large_content(content: str, max_tokens: int = 250000) -> List[str]:
    """
    Divide um conteÃºdo grande em partes menores baseado no limite de tokens.
    Tenta dividir em pontos naturais (seÃ§Ãµes markdown).
    """
    # Divide por seÃ§Ãµes (## ) primeiro
    # Se uma seÃ§Ã£o for muito grande, divide em subseÃ§Ãµes (### )
    # MantÃ©m a estrutura e contexto do documento
```

**CaracterÃ­sticas:**
- âœ… Divide em seÃ§Ãµes naturais do markdown (`##`, `###`)
- âœ… MantÃ©m o contexto semÃ¢ntico
- âœ… Deixa margem de seguranÃ§a (250k em vez de 300k)
- âœ… Adiciona metadata indicando qual parte do documento

### 3. **Processamento AutomÃ¡tico**

A funÃ§Ã£o `process_markdown_file()` foi atualizada para:

1. **Verificar o tamanho** do documento
2. **Dividir automaticamente** se necessÃ¡rio (>250k tokens)
3. **Processar cada parte** separadamente
4. **Combinar todos os chunks** no final

```python
def process_markdown_file(file_like) -> List[Document]:
    # ... lÃª o conteÃºdo ...
    
    # Verifica o tamanho
    token_count = count_tokens(content)
    print(f"ğŸ“Š Documento: {file_like.name} - {token_count:,} tokens")
    
    # Se muito grande, divide e processa em partes
    if token_count > 250000:
        print("âš ï¸ Documento muito grande! Processando em partes separadas...")
        content_parts = split_large_content(content, max_tokens=250000)
        
        all_chunks = []
        for i, part_content in enumerate(content_parts, 1):
            print(f"   Processando parte {i}/{len(content_parts)}...")
            part_chunks = _process_markdown_content(part_content, file_like.name, part_index=i)
            all_chunks.extend(part_chunks)
        
        return all_chunks
    else:
        # Documento normal
        return _process_markdown_content(content, file_like.name)
```

---

## ğŸ“Š Exemplo de SaÃ­da

Quando vocÃª processar o documento grande agora, verÃ¡:

```
ğŸ“Š Documento: Passo a passo - MÃ³dulo de Compras_documentacao_gerada.md - 338,717 tokens
âš ï¸ Documento muito grande! Processando em partes separadas...
âš ï¸ Documento muito grande (338717 tokens). Dividindo em partes menores...
âœ… Documento dividido em 2 partes
   Parte 1: 180,450 tokens
   Parte 2: 158,267 tokens
   Processando parte 1/2...
   Processando parte 2/2...
âœ… Total de 245 chunks gerados
```

---

## ğŸ¯ BenefÃ­cios

1. **âœ… Evita erro de limite de tokens** - Documentos grandes sÃ£o processados automaticamente
2. **âœ… MantÃ©m contexto semÃ¢ntico** - Divide em seÃ§Ãµes naturais do markdown
3. **âœ… Transparente para o usuÃ¡rio** - Funciona automaticamente, sem configuraÃ§Ã£o
4. **âœ… RastreÃ¡vel** - Metadata indica de qual parte cada chunk veio
5. **âœ… Eficiente** - Usa tiktoken para contagem precisa

---

## ğŸš€ Como Usar

NÃ£o hÃ¡ mudanÃ§a no uso! O sistema agora detecta e processa documentos grandes automaticamente:

```python
# Na interface Streamlit ou em qualquer lugar
uploaded_file = st.file_uploader("Upload documento", type=["md", "pdf", "txt"])

if uploaded_file:
    # Sistema detecta automaticamente se Ã© grande e divide
    chunks = process_markdown_file(uploaded_file)
    
    # Adiciona Ã  base vetorial normalmente
    vector_store.add_documents(chunks)
```

---

## ğŸ” VerificaÃ§Ã£o

Para verificar se a soluÃ§Ã£o estÃ¡ funcionando:

1. **Teste o mÃ³dulo:**
   ```bash
   python3 -c "from backend.processing import count_tokens, split_large_content; print('âœ… OK!')"
   ```

2. **Processe o documento problemÃ¡tico:**
   - Carregue o arquivo na interface Streamlit
   - O sistema mostrarÃ¡ mensagens de divisÃ£o se necessÃ¡rio
   - O documento serÃ¡ processado com sucesso

---

## ğŸ“ Arquivos Modificados

- âœ… `backend/processing.py` - Adicionadas funÃ§Ãµes de contagem e divisÃ£o de tokens
- âœ… `SOLUCAO_ERRO_TOKENS.md` - Este documento de referÃªncia

---

## ğŸ’¡ Dicas

1. **Para documentos menores** (<250k tokens): Nenhuma mudanÃ§a, processa normalmente
2. **Para documentos grandes** (>250k tokens): Divide automaticamente em partes
3. **Limite de seguranÃ§a**: 250k em vez de 300k para deixar margem
4. **SeÃ§Ãµes longas**: Se uma seÃ§Ã£o Ãºnica for muito grande, divide em subseÃ§Ãµes tambÃ©m

---

## âš¡ Performance

- **Antes**: âŒ Erro ao processar documentos >300k tokens
- **Depois**: âœ… Processa documentos de qualquer tamanho
- **Overhead**: MÃ­nimo - apenas contagem inicial de tokens
- **Qualidade**: Mantida - divisÃ£o respeita estrutura semÃ¢ntica

---

**âœ… SoluÃ§Ã£o implementada e testada com sucesso!**
