# ðŸ“š Guia Completo - DocumentaÃ§Ã£o TÃ©cnica do Sistema

DocumentaÃ§Ã£o tÃ©cnica detalhada de todas as funÃ§Ãµes e mÃ³dulos do Assistente Koper.

---

## ðŸ“‹ Ãndice

- [backend/config.py](#backendconfigpy)
- [backend/vector_store.py](#backendvector_storepy)
- [backend/processing.py](#backendprocessingpy)
- [backend/metadata_enhancer.py](#backendmetadata_enhancerpy)
- [backend/hybrid_retriever.py](#backendhybrid_retrieverpy)
- [backend/improved_prompts.py](#backendimproved_promptspy)
- [backend/qa.py](#backendqapy)
- [frontend/main.py](#frontendmainpy)
- [gerar_documentacao_video.py](#gerar_documentacao_videopy)

---

## backend/config.py

### DescriÃ§Ã£o

Arquivo de configuraÃ§Ã£o central com todas as constantes e parÃ¢metros do sistema.

### VariÃ¡veis

#### Modelo OpenAI

```python
DEFAULT_MODEL = "gpt-4o-mini"
```

- **O que faz**: Define o modelo GPT padrÃ£o para geraÃ§Ã£o de respostas
- **Valores possÃ­veis**: `"gpt-4o-mini"`, `"gpt-4"`, `"gpt-3.5-turbo"`
- **Custo**: gpt-4o-mini Ã© mais barato e rÃ¡pido

#### Temperatura

```python
TEMPERATURE = 0.3
```

- **O que faz**: Controla a criatividade das respostas
- **Range**: 0.0 (determinÃ­stico) a 2.0 (muito criativo)
- **Recomendado**: 0.3 para documentaÃ§Ã£o tÃ©cnica (baixa criatividade, alta precisÃ£o)

#### Modelo de Embeddings

```python
EMBEDDING_MODEL = "text-embedding-3-small"
```

- **O que faz**: Define o modelo para gerar vetores de documentos
- **DimensÃµes**: 1536 dimensÃµes
- **Custo**: $0.02 por 1M tokens

#### Chunking

```python
CHUNK_SIZE = 1200
CHUNK_OVERLAP = 200
```

- **CHUNK_SIZE**: Tamanho de cada pedaÃ§o de documento em caracteres
- **CHUNK_OVERLAP**: SobreposiÃ§Ã£o entre chunks para manter contexto
- **Por que 1200?**: Balance entre contexto e precisÃ£o (muito grande = perda de relevÃ¢ncia, muito pequeno = perda de contexto)

#### Retrieval

```python
K_DOCUMENTS = 6
LAMBDA_MULT = 0.7
```

- **K_DOCUMENTS**: Quantos documentos recuperar para compor o contexto
- **LAMBDA_MULT**: Balance no MMR (0 = mÃ¡xima diversidade, 1 = mÃ¡xima relevÃ¢ncia)

#### ChromaDB

```python
CHROMA_PERSIST_DIR = "./db"
COLLECTION_NAME = "koper_docs"
```

- **CHROMA_PERSIST_DIR**: Pasta onde o banco vetorial Ã© salvo
- **COLLECTION_NAME**: Nome da coleÃ§Ã£o no ChromaDB

---

## backend/vector_store.py

### DescriÃ§Ã£o

Gerencia o ChromaDB (banco de dados vetorial) onde os documentos sÃ£o armazenados.

### FunÃ§Ãµes

#### `create_vector_store(documents: List[Document]) -> Chroma`

**O que faz**: Cria ou atualiza o vector store com novos documentos

**ParÃ¢metros**:

- `documents`: Lista de objetos `Document` do LangChain

**Retorna**: Objeto `Chroma` (vector store)

**Como funciona**:

1. Cria embeddings model (OpenAI)
2. Verifica se o diretÃ³rio `./db` existe
3. Se existir: carrega o vector store existente e adiciona novos docs
4. Se nÃ£o existir: cria novo vector store do zero
5. Persiste no disco

**Exemplo**:

```python
from backend.vector_store import create_vector_store
from langchain.schema import Document

docs = [Document(page_content="Texto", metadata={"source": "arquivo.md"})]
vector_store = create_vector_store(docs)
```

#### `load_vector_store() -> Optional[Chroma]`

**O que faz**: Carrega um vector store existente do disco

**Retorna**: Objeto `Chroma` ou `None` se nÃ£o existir

**Quando usar**: Ao iniciar o sistema, para recuperar documentos jÃ¡ processados

**Exemplo**:

```python
from backend.vector_store import load_vector_store

vector_store = load_vector_store()
if vector_store:
    print("Vector store carregado com sucesso!")
else:
    print("Nenhum vector store encontrado. Carregue documentos primeiro.")
```

---

## backend/processing.py

### DescriÃ§Ã£o

Pipeline de processamento de documentos: leitura, parsing, chunking e extraÃ§Ã£o de metadados.

### FunÃ§Ãµes

#### `process_documents(file_paths: List[str]) -> Tuple[List[Document], dict]`

**O que faz**: Processa uma lista de arquivos e retorna documentos prontos para o vector store

**ParÃ¢metros**:

- `file_paths`: Lista de caminhos completos dos arquivos

**Retorna**:

- `documents`: Lista de `Document` objetos
- `stats`: DicionÃ¡rio com estatÃ­sticas (`{"total_chunks": 42, "total_documents": 7}`)

**Como funciona**:

1. **Para cada arquivo**:
   - Detecta tipo (MD, TXT, PDF)
   - LÃª o conteÃºdo
   - Extrai metadados (YouTube URL, tÃ­tulo, imagens)
   - Extrai timestamps JSON se disponÃ­vel
2. **Chunking**:
   - Divide documento em chunks de 1200 caracteres
   - MantÃ©m sobreposiÃ§Ã£o de 200 caracteres
   - Preserva quebras de parÃ¡grafo
3. **Enriquecimento**:
   - Adiciona metadados tÃ©cnicos (metadata_enhancer)
   - Adiciona timestamps JSON ao page_content de todos os chunks
   - Marca chunks com `has_timestamps='true'`

**Exemplo**:

```python
from backend.processing import process_documents

file_paths = ["docs/arquivo1.md", "docs/arquivo2.md"]
documents, stats = process_documents(file_paths)

print(f"Processados {stats['total_documents']} documentos")
print(f"Gerados {stats['total_chunks']} chunks")
```

#### `extract_youtube_url(content: str) -> Optional[str]`

**O que faz**: Extrai URL do YouTube do conteÃºdo markdown

**Procura por**:

- `[video:URL]`
- `[youtube:URL]`
- Links diretos do YouTube

**Retorna**: String com a URL ou `None`

**Exemplo**:

```python
content = "[video:https://www.youtube.com/watch?v=ABC123]"
url = extract_youtube_url(content)
# url = "https://www.youtube.com/watch?v=ABC123"
```

#### `extract_timestamps_from_document(content: str) -> Optional[dict]`

**O que faz**: Extrai timestamps do formato JSON no documento

**Formato esperado**:

```markdown
[VIDEO_TIMESTAMPS_DATA]
{
"Nome do VÃ­deo": [
{"start": "00:00", "end": "01:30", "line": "DescriÃ§Ã£o"}
]
}
[/VIDEO_TIMESTAMPS_DATA]
```

**Retorna**: DicionÃ¡rio Python ou `None`

**Exemplo**:

```python
content = """
[VIDEO_TIMESTAMPS_DATA]
{"Video 1": [{"start": "00:00", "end": "01:00", "line": "Intro"}]}
[/VIDEO_TIMESTAMPS_DATA]
"""
timestamps = extract_timestamps_from_document(content)
# timestamps = {"Video 1": [{"start": "00:00", "end": "01:00", "line": "Intro"}]}
```

---

## backend/metadata_enhancer.py

### DescriÃ§Ã£o

Adiciona metadados tÃ©cnicos aos documentos para melhorar a busca.

### FunÃ§Ãµes

#### `enhance_metadata(doc: Document, doc_index: int, total_docs: int) -> Document`

**O que faz**: Enriquece os metadados de um documento

**Adiciona**:

- `chunk_index`: PosiÃ§Ã£o do chunk no documento (0, 1, 2...)
- `total_chunks`: Total de chunks do documento
- `relative_position`: PosiÃ§Ã£o relativa ("start", "middle", "end")
- `content_type`: Tipo de conteÃºdo detectado
- `technical_terms`: Lista de termos tÃ©cnicos encontrados

**DetecÃ§Ã£o de Tipos**:

- `procedural`: Se contÃ©m palavras como "passo", "clique", "selecione"
- `conceptual`: Se contÃ©m "conceito", "definiÃ§Ã£o", "significa"
- `reference`: Se contÃ©m "tabela", "lista", "referÃªncia"
- `troubleshooting`: Se contÃ©m "erro", "problema", "soluÃ§Ã£o"

**Exemplo**:

```python
from langchain.schema import Document
from backend.metadata_enhancer import enhance_metadata

doc = Document(
    page_content="Para criar uma pasta, clique no botÃ£o `Nova Pasta`",
    metadata={"source": "arquivo.md"}
)

enhanced = enhance_metadata(doc, doc_index=0, total_docs=5)

print(enhanced.metadata)
# {
#   "source": "arquivo.md",
#   "chunk_index": 0,
#   "total_chunks": 5,
#   "relative_position": "start",
#   "content_type": "procedural",
#   "technical_terms": ["criar", "pasta", "botÃ£o"]
# }
```

---

## backend/hybrid_retriever.py

### DescriÃ§Ã£o

Implementa busca hÃ­brida combinando busca semÃ¢ntica (vetores) e busca por keywords (BM25).

### Classe `HybridRetriever`

#### `__init__(vector_store: Chroma, k: int = 6, alpha: float = 0.7)`

**ParÃ¢metros**:

- `vector_store`: ChromaDB vector store
- `k`: NÃºmero de documentos a recuperar
- `alpha`: Peso da busca semÃ¢ntica (0.7 = 70% semÃ¢ntica, 30% keyword)

#### `get_relevant_documents(query: str) -> List[Document]`

**O que faz**: Busca documentos relevantes usando estratÃ©gia hÃ­brida

**Como funciona**:

1. **Busca SemÃ¢ntica** (70%):
   - Converte query em embedding
   - Busca por similaridade de cosseno
   - Recupera top-k documentos mais similares
2. **Busca por Keywords** (30%):
   - Usa algoritmo BM25
   - Busca por palavras exatas da query
   - Recupera top-k documentos mais relevantes
3. **CombinaÃ§Ã£o**:
   - Normaliza scores de ambas as buscas
   - Combina com pesos (0.7 e 0.3)
   - Ordena por score final
   - Remove duplicatas
   - Retorna top-k documentos

**Exemplo**:

```python
from backend.hybrid_retriever import HybridRetriever
from backend.vector_store import load_vector_store

vector_store = load_vector_store()
retriever = HybridRetriever(vector_store, k=6, alpha=0.7)

docs = retriever.get_relevant_documents("Como criar pasta?")
# Retorna 6 documentos mais relevantes
```

**Por que hÃ­brido?**:

- **SemÃ¢ntica**: Captura intenÃ§Ã£o e contexto ("como fazer X" = "procedimento para X")
- **Keywords**: Encontra termos especÃ­ficos ("botÃ£o Salvar" deve conter exatamente "Salvar")
- **Combinado**: Melhor precisÃ£o e cobertura

---

## backend/improved_prompts.py

### DescriÃ§Ã£o

Sistema de prompts adaptativos que detecta o tipo de pergunta e ajusta o prompt.

### FunÃ§Ãµes

#### `detect_prompt_type(query: str) -> str`

**O que faz**: Analisa a query e classifica em uma categoria

**Categorias**:

- `procedural`: "Como fazer X", "Passos para Y"
- `conceptual`: "O que Ã© X", "DefiniÃ§Ã£o de Y"
- `troubleshooting`: "Erro X", "Problema Y", "NÃ£o funciona"
- `comparison`: "DiferenÃ§a entre X e Y", "X vs Y"
- `general`: Outros casos

**Exemplo**:

```python
from backend.improved_prompts import detect_prompt_type

tipo = detect_prompt_type("Como criar uma pasta?")
# tipo = "procedural"

tipo = detect_prompt_type("O que Ã© o mÃ³dulo de compras?")
# tipo = "conceptual"

tipo = detect_prompt_type("Deu erro ao salvar")
# tipo = "troubleshooting"
```

#### `get_prompt_by_type(prompt_type: str) -> str`

**O que faz**: Retorna o prompt otimizado para cada tipo

**Prompts DisponÃ­veis**:

1. **PROCEDURAL_PROMPT**:

```
VocÃª Ã© um assistente tÃ©cnico especializado.
ForneÃ§a instruÃ§Ãµes passo a passo CLARAS e OBJETIVAS.
Use listas numeradas.
Destaque botÃµes com `cÃ³digo`.
```

2. **CONCEPTUAL_PROMPT**:

```
VocÃª Ã© um professor tÃ©cnico.
Explique conceitos de forma CLARA e DIDÃTICA.
Use analogias quando apropriado.
Defina termos tÃ©cnicos.
```

3. **TROUBLESHOOTING_PROMPT**:

```
VocÃª Ã© um especialista em suporte tÃ©cnico.
Identifique a CAUSA RAIZ do problema.
ForneÃ§a SOLUÃ‡Ã•ES especÃ­ficas e testadas.
Liste possÃ­veis causas.
```

4. **COMPARISON_PROMPT**:

```
VocÃª Ã© um analista tÃ©cnico.
Compare de forma OBJETIVA e ESTRUTURADA.
Use tabelas quando possÃ­vel.
Destaque diferenÃ§as principais.
```

**Exemplo**:

```python
from backend.improved_prompts import get_prompt_by_type

prompt = get_prompt_by_type("procedural")
# Retorna prompt otimizado para procedimentos
```

---

## backend/qa.py

### DescriÃ§Ã£o

MÃ³dulo principal de QA (Questions & Answers) com RAG chain e seleÃ§Ã£o de timestamps.

### FunÃ§Ãµes

#### `ask_question(query, vector_store, model_name=None, chat_history=None, system_prompt=None, temperature=None) -> dict`

**O que faz**: Processa uma pergunta e retorna resposta com contexto

**ParÃ¢metros**:

- `query`: Pergunta do usuÃ¡rio (string)
- `vector_store`: ChromaDB vector store
- `model_name`: Modelo GPT (opcional, usa default)
- `chat_history`: HistÃ³rico de conversa (opcional)
- `system_prompt`: Prompt customizado (opcional, detecta automaticamente)
- `temperature`: Temperatura do modelo (opcional, usa default)

**Retorna**:

```python
{
    "answer": "Resposta formatada em Markdown...",
    "source_documents": [doc1, doc2, doc3, ...]
}
```

**Fluxo Completo**:

1. **ConfiguraÃ§Ã£o**:

   - Usa valores default se nÃ£o fornecidos
   - Detecta tipo de prompt baseado na query
   - Cria modelo LLM (ChatOpenAI)

2. **Retrieval (Busca)**:

   - Cria retriever MMR do vector store
   - ParÃ¢metros: `k=6`, `lambda_mult=0.7`
   - Busca documentos relevantes

3. **Contexto**:
   - Para cada documento recuperado:
     - Extrai source name
     - Extrai YouTube URL se disponÃ­vel
     - Extrai timestamps JSON se disponÃ­vel
     - Remove timestamps do conteÃºdo para o LLM
     - Monta string de contexto
4. **GeraÃ§Ã£o de Resposta**:

   - Monta prompt com: system_prompt + contexto + query
   - Chama GPT-4
   - Recebe resposta em Markdown

5. **PÃ³s-processamento**:

   - **Se hÃ¡ YouTube URL**:
     - Procura timestamps do documento mais relevante
     - Calcula score de relevÃ¢ncia para cada timestamp:
       - +1 ponto: cada palavra da query (>3 chars) que aparece em `line`
       - +10 pontos: se `line` aparece no conteÃºdo do chunk
     - Seleciona timestamp com maior score
     - Converte timestamp para segundos
     - Cria iframe do YouTube com `?start=SEGUNDOS`
     - Adiciona ao final da resposta

6. **Retorno**:
   - DicionÃ¡rio com `answer` e `source_documents`

**Exemplo**:

```python
from backend.qa import ask_question
from backend.vector_store import load_vector_store

vector_store = load_vector_store()
result = ask_question(
    query="Como criar uma pasta?",
    vector_store=vector_store
)

print(result["answer"])
# Resposta formatada com iframe do vÃ­deo se disponÃ­vel
```

#### SeleÃ§Ã£o Inteligente de Timestamps

**Algoritmo**:

```python
for timestamp in timestamps:
    score = 0

    # Score 1: Palavras da query na descriÃ§Ã£o
    for word in query.split():
        if len(word) > 3 and word.lower() in timestamp['line'].lower():
            score += 1

    # Score 2: DescriÃ§Ã£o aparece no chunk relevante
    if timestamp['line'][:50] in first_doc_content.lower():
        score += 10

    if score > best_score:
        best_score = score
        best_timestamp = timestamp
```

**Exemplo PrÃ¡tico**:

```
Query: "onde estÃ¡ o botÃ£o para criar pasta?"

Timestamps disponÃ­veis:
1. {"start": "00:01", "line": "IntroduÃ§Ã£o ao mÃ³dulo de armazenamento"}
   â†’ Score: 0 (nenhuma palavra relevante)

2. {"start": "02:35", "line": "Como criar e gerenciar pastas no sistema"}
   â†’ Score: 3 ("criar" + "pasta" + "gerenciar" na descriÃ§Ã£o)

Selecionado: Timestamp 2 (02:35) âœ…
```

---

## frontend/main.py

### DescriÃ§Ã£o

Interface Streamlit com 2 abas: Chat e Upload de Documentos.

### Estrutura

#### Session State

```python
st.session_state.docs_loaded = False  # Documentos carregados?
st.session_state.vector_store = None  # ChromaDB instance
st.session_state.messages = []        # HistÃ³rico do chat
```

#### Menu Lateral

- Logo do Koper
- TÃ­tulo do assistente
- BotÃ£o "Limpar HistÃ³rico"
- Perguntas sugeridas (6 exemplos)

#### Aba 1: ðŸ’¬ Chat

**Componentes**:

1. **VerificaÃ§Ã£o**: Checa se `docs_loaded == True`
2. **HistÃ³rico**: Exibe todas as mensagens (`st.chat_message`)
3. **Input**: Campo de texto (`st.chat_input`)
4. **Processamento**:
   - Adiciona mensagem do usuÃ¡rio ao histÃ³rico
   - Chama `ask_question()`
   - Adiciona resposta ao histÃ³rico
   - `st.rerun()` para atualizar interface

**Fluxo**:

```
UsuÃ¡rio digita â†’ Adiciona ao histÃ³rico â†’ ask_question() â†’
Adiciona resposta â†’ st.rerun() â†’ Interface atualizada
```

#### Aba 2: ðŸ“¤ Upload de Documentos

**OpÃ§Ã£o A: Carregar pasta docs/**

```python
def load_docs_folder():
    file_paths = [f"docs/{f}" for f in os.listdir("docs") if f.endswith(".md")]
    documents, stats = process_documents(file_paths)
    vector_store = create_vector_store(documents)
    st.session_state.vector_store = vector_store
    st.session_state.docs_loaded = True
```

**OpÃ§Ã£o B: Upload manual**

```python
uploaded_files = st.file_uploader("Arraste arquivos aqui", type=["md", "txt", "pdf"])
def process_uploaded_files(files):
    # Salva arquivos temporÃ¡rios
    # Processa com process_documents()
    # Atualiza vector_store
```

### FunÃ§Ãµes Auxiliares

#### `load_docs_folder() -> Tuple[bool, str]`

- Lista arquivos `.md` em `docs/`
- Processa todos com `process_documents()`
- Cria/atualiza vector store
- Retorna `(sucesso, mensagem)`

#### `process_uploaded_files(files) -> Tuple[bool, str]`

- Salva arquivos temporÃ¡rios
- Processa com `process_documents()`
- Cria/atualiza vector store
- Retorna `(sucesso, mensagem)`

---

## gerar_documentacao_video.py

### DescriÃ§Ã£o

Script para gerar documentaÃ§Ã£o estruturada a partir de vÃ­deos do YouTube.

### FunÃ§Ãµes Principais

#### `extrair_video_id(url: str) -> Optional[str]`

- Extrai ID do vÃ­deo de URLs do YouTube
- Suporta formatos: `watch?v=ID`, `youtu.be/ID`, etc.

#### `obter_transcricao(video_id: str) -> List[dict]`

- Usa `youtube-transcript-api`
- Baixa transcriÃ§Ã£o em portuguÃªs
- Retorna lista de entries: `[{"start": 0.0, "duration": 2.5, "text": "..."}]`

#### `dividir_em_chunks(transcript, intervalo_segundos=60) -> List[dict]`

- Agrupa transcriÃ§Ã£o em chunks temporais
- Cada chunk: `{"start": 0.0, "end": 60.0, "text": "..."}`
- PadrÃ£o: chunks de 60 segundos

#### `gerar_secao_com_ia(chunk, client, video_title) -> str`

- Envia chunk para GPT-4
- Gera seÃ§Ã£o Markdown estruturada
- Retorna texto formatado com tÃ­tulo e conteÃºdo

#### `gerar_timestamps_json(chunks, video_title) -> dict`

- Converte chunks em formato JSON de timestamps
- Cria descriÃ§Ãµes a partir do texto (primeiros 100 chars)
- Retorna: `{"Video Title": [{"start": "00:00", "end": "01:00", "line": "..."}]}`

#### `segundos_para_timestamp(segundos: float) -> str`

- Converte segundos para formato `HH:MM:SS` ou `MM:SS`
- Exemplo: `125.5` â†’ `"02:05"`

### Fluxo de ExecuÃ§Ã£o

```
1. UsuÃ¡rio informa URL do YouTube
2. Extrai video_id
3. Baixa transcriÃ§Ã£o
4. Divide em chunks (60s cada)
5. Para cada chunk:
   - Envia para GPT-4
   - Gera seÃ§Ã£o Markdown
6. Cria JSON de timestamps
7. Salva arquivo em docs/
```

**Exemplo de Output**:

```markdown
# Passo a passo - Tutorial XYZ

[video:https://www.youtube.com/watch?v=ABC123]

## IntroduÃ§Ã£o ao Sistema

ConteÃºdo gerado pela IA...

## Criando Registros

Mais conteÃºdo...

[VIDEO_TIMESTAMPS_DATA]
{
"Tutorial XYZ": [
{"start": "00:00", "end": "01:00", "line": "IntroduÃ§Ã£o..."},
{"start": "01:00", "end": "02:00", "line": "Criando..."}
]
}
[/VIDEO_TIMESTAMPS_DATA]
```

---

## ðŸ”§ Fluxo Completo do Sistema

### 1. Processamento de Documentos

```
Arquivo .md â†’ process_documents() â†’
  â†“
  - Extrai YouTube URL
  - Extrai timestamps JSON
  - Divide em chunks (1200 chars, overlap 200)
  - Adiciona metadados (metadata_enhancer)
  - Adiciona timestamps a TODOS os chunks
  â†“
Documents â†’ create_vector_store() â†’
  â†“
  - Gera embeddings (OpenAI)
  - Armazena no ChromaDB
  â†“
Vector Store salvo em ./db
```

### 2. Query & Response

```
UsuÃ¡rio faz pergunta â†’
  â†“
ask_question() â†’
  â†“
  1. DetecÃ§Ã£o de tipo de prompt
  2. Retrieval (MMR, k=6)
  3. ExtraÃ§Ã£o de timestamps dos docs recuperados
  4. Montagem de contexto
  5. Chamada GPT-4
  6. SeleÃ§Ã£o inteligente de timestamp:
     - Calcula score para cada timestamp
     - Seleciona com maior score
  7. ConversÃ£o para segundos
  8. CriaÃ§Ã£o de iframe do YouTube
  â†“
Resposta com vÃ­deo embedado â†’ Interface Streamlit
```

### 3. SeleÃ§Ã£o de Timestamp

```
Documentos recuperados â†’
  â†“
Extrai timestamps do doc mais relevante â†’
  â†“
Para cada timestamp:
  - Score baseado em palavras da query
  - Bonus se descriÃ§Ã£o aparece no chunk
  â†“
Seleciona timestamp com maior score â†’
  â†“
Converte "MM:SS" para segundos â†’
  â†“
Cria URL: youtube.com/embed/ID?start=SEGUNDOS â†’
  â†“
Iframe HTML adicionado Ã  resposta
```

---

## ðŸ“Š MÃ©tricas e Performance

### Chunking

- **Tamanho**: 1200 caracteres
- **Overlap**: 200 caracteres
- **Documentos tÃ­picos**: 5-10 chunks por arquivo
- **Total no sistema**: ~2500 chunks

### Retrieval

- **Documentos recuperados**: 6
- **EstratÃ©gia**: MMR (Maximal Marginal Relevance)
- **Lambda**: 0.7 (70% relevÃ¢ncia, 30% diversidade)

### Embeddings

- **Modelo**: text-embedding-3-small
- **DimensÃµes**: 1536
- **Custo**: $0.02 por 1M tokens

### LLM

- **Modelo**: gpt-4o-mini
- **Temperatura**: 0.3
- **Tokens mÃ©dios por resposta**: ~500-800
- **Custo**: $0.15 por 1M tokens input, $0.60 por 1M tokens output

---

## ðŸš€ OtimizaÃ§Ãµes Implementadas

### 1. Busca HÃ­brida

- Combina semÃ¢ntica (70%) + keywords (30%)
- Melhor recall e precision

### 2. Timestamps nos Chunks

- Todos os chunks do documento incluem timestamps
- NÃ£o precisa achar chunk especÃ­fico com tag `[video:]`

### 3. SeleÃ§Ã£o Inteligente

- Score baseado em relevÃ¢ncia
- NÃ£o usa sempre o primeiro timestamp

### 4. MMR Retrieval

- Evita documentos muito similares
- Aumenta diversidade no contexto

### 5. Prompts Adaptativos

- Detecta tipo de pergunta
- Usa prompt especializado

---

**DocumentaÃ§Ã£o atualizada em**: Novembro 2025
