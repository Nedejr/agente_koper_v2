#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de teste para validar processamento de documentos grandes
"""

import os
import sys
from pathlib import Path

# Adiciona o diretÃ³rio do projeto ao path
sys.path.insert(0, str(Path(__file__).parent))

from backend.processing import process_markdown_file, count_tokens
import io


def test_large_document():
    """Testa o processamento de um documento grande"""
    
    print("ğŸ§ª Teste de Processamento de Documentos Grandes")
    print("=" * 70)
    
    # Caminho para o documento problemÃ¡tico
    doc_path = "docs/Passo a passo - MÃ³dulo de Compras_documentacao_gerada.md"
    
    if not os.path.exists(doc_path):
        print(f"âŒ Arquivo nÃ£o encontrado: {doc_path}")
        print("\nğŸ’¡ Teste com documento de exemplo grande...")
        
        # Cria um documento grande de teste
        test_content = """# DocumentaÃ§Ã£o de Teste

## SeÃ§Ã£o 1

""" + ("Este Ã© um parÃ¡grafo de teste. " * 1000 + "\n\n") * 100
        
        # Cria um objeto file-like
        file_like = io.BytesIO(test_content.encode('utf-8'))
        file_like.name = "teste_grande.md"
        
        print(f"ğŸ“„ Documento de teste criado")
        print(f"ğŸ“Š Tamanho: {len(test_content):,} caracteres")
        print(f"ğŸ“Š Estimativa: ~{len(test_content) // 4:,} tokens")
        
    else:
        print(f"ğŸ“„ Processando: {doc_path}")
        
        # LÃª o arquivo real
        with open(doc_path, 'rb') as f:
            content = f.read()
        
        file_like = io.BytesIO(content)
        file_like.name = os.path.basename(doc_path)
        
        print(f"ğŸ“Š Tamanho: {len(content):,} bytes")
        
    print("\nğŸ”„ Iniciando processamento...")
    print("-" * 70)
    
    try:
        # Processa o arquivo
        chunks = process_markdown_file(file_like)
        
        print("-" * 70)
        print(f"\nâœ… SUCESSO!")
        print(f"ğŸ“¦ Total de chunks gerados: {len(chunks)}")
        
        # EstatÃ­sticas
        if chunks:
            avg_size = sum(len(c.page_content) for c in chunks) // len(chunks)
            print(f"ğŸ“ Tamanho mÃ©dio por chunk: {avg_size:,} caracteres")
            
            # Verifica metadados
            has_parts = any('part' in c.metadata for c in chunks)
            if has_parts:
                parts = set(c.metadata.get('part', 0) for c in chunks if 'part' in c.metadata)
                print(f"ğŸ“‘ Documento dividido em {len(parts)} partes")
            
            print(f"\nğŸ“‹ Exemplo de metadados do primeiro chunk:")
            print(f"   {chunks[0].metadata}")
            
            print(f"\nğŸ“ Primeiros 200 caracteres do primeiro chunk:")
            print(f"   {chunks[0].page_content[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ERRO: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("\n")
    success = test_large_document()
    print("\n" + "=" * 70)
    
    if success:
        print("ğŸ‰ Teste concluÃ­do com sucesso!")
        print("\nğŸ’¡ PrÃ³ximos passos:")
        print("   1. Execute a aplicaÃ§Ã£o Streamlit: streamlit run frontend/main.py")
        print("   2. FaÃ§a upload do documento grande")
        print("   3. O sistema processarÃ¡ automaticamente em partes")
    else:
        print("âš ï¸  Teste falhou. Verifique os erros acima.")
    
    print("=" * 70)
    print()
